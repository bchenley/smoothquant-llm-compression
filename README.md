# smoothquant-llm-compression
Post-training quantization of LLMs using SmoothQuant for faster, cheaper inference
